{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70db946",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9b0d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (1500, 1764)\n",
      "Label array shape: (1500,)\n",
      "Fold 1 accuracy: 0.973333\n",
      "Fold 2 accuracy: 1.000000\n",
      "Fold 3 accuracy: 0.993333\n",
      "Fold 4 accuracy: 0.966667\n",
      "Fold 5 accuracy: 0.973333\n",
      "Fold 6 accuracy: 0.986667\n",
      "Fold 7 accuracy: 0.980000\n",
      "Fold 8 accuracy: 0.986667\n",
      "Fold 9 accuracy: 0.986667\n",
      "Fold 10 accuracy: 0.980000\n",
      "Average accuracy: 0.9826666666666668\n",
      "Standard deviation: 0.009521904571390453\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the image directory and HOG parameters\n",
    "img_dir = \"C:/WPy64-31110/notebooks/docs/Shoe vs Sandal vs Boot Image Dataset/\"\n",
    "classes = ['boot', 'sandal', 'shoe']\n",
    "winSize = (64,64)\n",
    "blockSize = (16,16)\n",
    "blockStride = (8,8)\n",
    "cellSize = (8,8)\n",
    "nbins = 9\n",
    "\n",
    "# Create the HOG descriptor object\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each image in the directory and extract features\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\")):\n",
    "            # Load the image and resize to a fixed size\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, winSize)\n",
    "\n",
    "                # Compute the HOG descriptor for the image and store in the feature list\n",
    "                features = hog.compute(img)\n",
    "                X.append(features.ravel())\n",
    "\n",
    "                # Extract the label from the filename and store in the label list\n",
    "                label = class_name\n",
    "                y.append(label)\n",
    "\n",
    "# Convert the feature and label lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Feature array shape:\", X.shape)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train an SVM classifier on the training set\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Print the accuracy for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation:\", np.std(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd4f1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[57  0  1]\n",
      " [ 0 40  1]\n",
      " [ 0  2 49]]\n",
      "Fold 2 accuracy: 1.000000\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 0 45  0]\n",
      " [ 0  0 59]]\n",
      "Fold 3 accuracy: 0.993333\n",
      "Confusion matrix:\n",
      " [[44  0  0]\n",
      " [ 0 54  1]\n",
      " [ 0  0 51]]\n",
      "Fold 4 accuracy: 0.966667\n",
      "Confusion matrix:\n",
      " [[50  0  0]\n",
      " [ 0 53  0]\n",
      " [ 0  5 42]]\n",
      "Fold 5 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[51  2  0]\n",
      " [ 0 57  0]\n",
      " [ 1  1 38]]\n",
      "Fold 6 accuracy: 0.986667\n",
      "Confusion matrix:\n",
      " [[51  0  0]\n",
      " [ 0 50  0]\n",
      " [ 1  1 47]]\n",
      "Fold 7 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[62  0  0]\n",
      " [ 0 46  2]\n",
      " [ 0  1 39]]\n",
      "Fold 8 accuracy: 0.986667\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 0 55  1]\n",
      " [ 0  1 47]]\n",
      "Fold 9 accuracy: 0.986667\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 1 43  1]\n",
      " [ 0  0 65]]\n",
      "Fold 10 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[49  0  1]\n",
      " [ 0 49  1]\n",
      " [ 0  1 49]]\n",
      "Overall performance metrics:\n",
      "Accuracy: 0.9826666666666667\n",
      "Confusion matrix:\n",
      " [[496   2   2]\n",
      " [  1 492   7]\n",
      " [  2  12 486]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        boot       0.99      0.99      0.99       500\n",
      "      sandal       0.97      0.98      0.98       500\n",
      "        shoe       0.98      0.97      0.98       500\n",
      "\n",
      "    accuracy                           0.98      1500\n",
      "   macro avg       0.98      0.98      0.98      1500\n",
      "weighted avg       0.98      0.98      0.98      1500\n",
      "\n",
      "Cohen's Kappa: 0.974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "\n",
    "# Create empty lists to store the true labels and predicted labels for all folds\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train an SVM classifier on the training set\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Add the true and predicted labels to the lists for all folds\n",
    "    y_true_all.extend(y_val)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "    # Print the accuracy and confusion matrix for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Calculate and print the overall performance metrics\n",
    "print(\"Overall performance metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_true_all, y_pred_all))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true_all, y_pred_all))\n",
    "print(classification_report(y_true_all, y_pred_all))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_true_all, y_pred_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92a38090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: [1.   0.99 0.98]\n",
      "Sensitivity: [0.98 0.98 0.98]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred, labels=classes)\n",
    "\n",
    "# Calculate the true positives, false positives, true negatives, and false negatives\n",
    "TP = np.diag(cm)\n",
    "FP = cm.sum(axis=0) - TP\n",
    "FN = cm.sum(axis=1) - TP\n",
    "TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "# Calculate the specificity and sensitivity\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"Sensitivity:\", sensitivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "61d97b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 0.1\n",
      "Best cross-validation score: 0.9653333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Create a GridSearchCV object with a linear SVM classifier and the hyperparameter grid\n",
    "clf = GridSearchCV(SVC(kernel='linear'), param_grid, cv=10)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding cross-validation score\n",
    "print(\"Best C value:\", clf.best_params_['C'])\n",
    "print(\"Best cross-validation score:\", clf.best_score_) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3ec43",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d55e0345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (1500, 1764)\n",
      "Label array shape: (1500,)\n",
      "Fold 1 accuracy: 0.973333\n",
      "Fold 2 accuracy: 0.986667\n",
      "Fold 3 accuracy: 0.966667\n",
      "Fold 4 accuracy: 0.946667\n",
      "Fold 5 accuracy: 0.953333\n",
      "Fold 6 accuracy: 0.966667\n",
      "Fold 7 accuracy: 0.940000\n",
      "Fold 8 accuracy: 0.973333\n",
      "Fold 9 accuracy: 0.953333\n",
      "Fold 10 accuracy: 0.980000\n",
      "Average accuracy: 0.9640000000000001\n",
      "Standard deviation: 0.014360439485692024\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the image directory and HOG parameters\n",
    "img_dir = \"C:/WPy64-31110/notebooks/docs/Shoe vs Sandal vs Boot Image Dataset/\"\n",
    "classes = ['boot', 'sandal', 'shoe']\n",
    "winSize = (64,64)\n",
    "blockSize = (16,16)\n",
    "blockStride = (8,8)\n",
    "cellSize = (8,8)\n",
    "nbins = 9\n",
    "\n",
    "# Create the HOG descriptor object\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each image in the directory and extract features\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\")):\n",
    "            # Load the image and resize to a fixed size\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, winSize)\n",
    "\n",
    "                # Compute the HOG descriptor for the image and store in the feature list\n",
    "                features = hog.compute(img)\n",
    "                X.append(features.ravel())\n",
    "\n",
    "                # Extract the label from the filename and store in the label list\n",
    "                label = class_name\n",
    "                y.append(label)\n",
    "\n",
    "# Convert the feature and label lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Feature array shape:\", X.shape)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train a KNN classifier on the training set\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Print the accuracy for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "\n",
    "# Print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation:\", np.std(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "237526c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[57  0  1]\n",
      " [ 0 39  2]\n",
      " [ 1  0 50]]\n",
      "Fold 2 accuracy: 0.986667\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 0 44  1]\n",
      " [ 1  0 58]]\n",
      "Fold 3 accuracy: 0.966667\n",
      "Confusion matrix:\n",
      " [[44  0  0]\n",
      " [ 0 54  1]\n",
      " [ 2  2 47]]\n",
      "Fold 4 accuracy: 0.946667\n",
      "Confusion matrix:\n",
      " [[50  0  0]\n",
      " [ 2 50  1]\n",
      " [ 2  3 42]]\n",
      "Fold 5 accuracy: 0.953333\n",
      "Confusion matrix:\n",
      " [[52  1  0]\n",
      " [ 3 53  1]\n",
      " [ 2  0 38]]\n",
      "Fold 6 accuracy: 0.966667\n",
      "Confusion matrix:\n",
      " [[51  0  0]\n",
      " [ 0 47  3]\n",
      " [ 2  0 47]]\n",
      "Fold 7 accuracy: 0.940000\n",
      "Confusion matrix:\n",
      " [[62  0  0]\n",
      " [ 2 44  2]\n",
      " [ 5  0 35]]\n",
      "Fold 8 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 1 54  1]\n",
      " [ 1  1 46]]\n",
      "Fold 9 accuracy: 0.953333\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 2 42  1]\n",
      " [ 4  0 61]]\n",
      "Fold 10 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[50  0  0]\n",
      " [ 0 48  2]\n",
      " [ 0  1 49]]\n",
      "Overall performance metrics:\n",
      "Accuracy: 0.964\n",
      "Confusion matrix:\n",
      " [[498   1   1]\n",
      " [ 10 475  15]\n",
      " [ 20   7 473]]\n",
      "Specificity: [1.   0.99 0.98]\n",
      "Sensitivity: [1.   0.96 0.98]\n",
      "Cohen's Kappa: 0.946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "\n",
    "# Create empty lists to store the true labels and predicted labels for all folds\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train a KNN classifier on the training set\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Add the true and predicted labels to the lists for all folds\n",
    "    y_true_all.extend(y_val)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "    # Print the accuracy and confusion matrix for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Calculate and print the overall performance metrics\n",
    "print(\"Overall performance metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_true_all, y_pred_all))\n",
    "\n",
    "# Get the confusion matrix for all data\n",
    "cm_all = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion matrix:\\n\", cm_all)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred, labels=classes)\n",
    "\n",
    "# Calculate the true positives, false positives, true negatives, and false negatives\n",
    "TP = np.diag(cm)\n",
    "FP = cm.sum(axis=0) - TP\n",
    "FN = cm.sum(axis=1) - TP\n",
    "TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "# Calculate the specificity and sensitivity\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_true_all, y_pred_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a37278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
      "Best cross-validation score: 0.9573333333333334\n",
      "Test set score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the range of hyperparameters to explore\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "}\n",
    "\n",
    "# Create the KNN classifier object\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=10)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding performance metrics\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "print(\"Test set score:\", grid_search.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cca12e",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eeae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (1500, 64, 64, 3)\n",
      "Label array shape: (1500,)\n",
      "Test accuracy: 0.95333331823349\n",
      "Fold 1 accuracy: 0.953333\n",
      "Fold 2 accuracy: 0.993333\n",
      "Fold 3 accuracy: 1.000000\n",
      "Fold 4 accuracy: 1.000000\n",
      "Fold 5 accuracy: 1.000000\n",
      "Fold 6 accuracy: 1.000000\n",
      "Fold 7 accuracy: 1.000000\n",
      "Fold 8 accuracy: 1.000000\n",
      "Fold 9 accuracy: 1.000000\n",
      "Fold 10 accuracy: 1.000000\n",
      "Average accuracy: 0.9946666657924652\n",
      "Standard deviation: 0.013920413101189005\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the image directory and CNN parameters\n",
    "img_dir = \"C:/WPy64-31110/notebooks/docs/Shoe vs Sandal vs Boot Image Dataset/\"\n",
    "classes = ['boot', 'sandal', 'shoe']\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Create a CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(classes))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each image in the directory and extract features\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\")):\n",
    "            # Load the image and resize to a fixed size\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = keras.preprocessing.image.load_img(\n",
    "                img_path, target_size=(img_height, img_width)\n",
    "            )\n",
    "            img_array = keras.preprocessing.image.img_to_array(img)\n",
    "            X.append(img_array)\n",
    "\n",
    "            # Extract the label from the filename and store in the label list\n",
    "            label = classes.index(class_name)\n",
    "            y.append(label)\n",
    "\n",
    "# Convert the feature and label lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Feature array shape:\", X.shape)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Print the accuracy for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "\n",
    "# Print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation:\", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4654360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 78ms/step\n",
      "5/5 [==============================] - 0s 74ms/step\n",
      "5/5 [==============================] - 1s 88ms/step\n",
      "5/5 [==============================] - 0s 75ms/step\n",
      "5/5 [==============================] - 0s 76ms/step\n",
      "5/5 [==============================] - 0s 77ms/step\n",
      "5/5 [==============================] - 0s 75ms/step\n",
      "5/5 [==============================] - 0s 74ms/step\n",
      "5/5 [==============================] - 0s 76ms/step\n",
      "5/5 [==============================] - 0s 77ms/step\n",
      "Overall classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        boot       1.00      1.00      1.00       500\n",
      "      sandal       1.00      1.00      1.00       500\n",
      "        shoe       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      1500\n",
      "   macro avg       1.00      1.00      1.00      1500\n",
      "weighted avg       1.00      1.00      1.00      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize lists to store the true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    true_labels.extend(y_val)\n",
    "    predicted_labels.extend(y_pred_classes)\n",
    "\n",
    "\n",
    "# Calculate the overall performance metrics\n",
    "print(\"Overall classification report:\\n\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=classes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faeaa1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 47ms/step\n",
      "5/5 [==============================] - 0s 39ms/step\n",
      "5/5 [==============================] - 0s 39ms/step\n",
      "5/5 [==============================] - 0s 38ms/step\n",
      "5/5 [==============================] - 0s 46ms/step\n",
      "5/5 [==============================] - 0s 44ms/step\n",
      "5/5 [==============================] - 0s 41ms/step\n",
      "5/5 [==============================] - 0s 58ms/step\n",
      "5/5 [==============================] - 0s 40ms/step\n",
      "5/5 [==============================] - 0s 37ms/step\n",
      "Overall accuracy: 1.0\n",
      "Cohen's Kappa: 1.0\n",
      "Confusion matrix:\n",
      "[[500   0   0]\n",
      " [  0 500   0]\n",
      " [  0   0 500]]\n",
      "Specificity: [1.0, 1.0, 1.0]\n",
      "Sensitivity: [1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    " from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import KFold\n",
    "# Use 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty lists to store the true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict the labels for the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Append the true and predicted labels to the lists\n",
    "    true_labels.extend(y_val)\n",
    "    predicted_labels.extend(y_pred_classes)\n",
    "\n",
    "# Calculate the overall accuracy, F1 score, and Cohen's Kappa\n",
    "overall_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate the specificity and sensitivity for each class\n",
    "num_classes = len(classes)\n",
    "specificity = []\n",
    "sensitivity = []\n",
    "for i in range(num_classes):\n",
    "    tp = cm[i][i]\n",
    "    fp = np.sum(cm[:, i]) - tp\n",
    "    fn = np.sum(cm[i, :]) - tp\n",
    "    tn = np.sum(cm) - tp - fp - fn\n",
    "    specificity.append(tn / (tn + fp))\n",
    "    sensitivity.append(tp / (tp + fn))\n",
    "\n",
    "# Print the results\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"Sensitivity:\", sensitivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f03609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7b58ea",
   "metadata": {},
   "source": [
    "# FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf1a080d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (1500, 1764)\n",
      "Label array shape: (1500,)\n",
      "Fold 1 accuracy: 0.873333\n",
      "Fold 2 accuracy: 0.873333\n",
      "Fold 3 accuracy: 0.880000\n",
      "Fold 4 accuracy: 0.906667\n",
      "Fold 5 accuracy: 0.893333\n",
      "Fold 6 accuracy: 0.906667\n",
      "Fold 7 accuracy: 0.906667\n",
      "Fold 8 accuracy: 0.900000\n",
      "Fold 9 accuracy: 0.886667\n",
      "Fold 10 accuracy: 0.900000\n",
      "Average accuracy: 0.8926666676998138\n",
      "Standard deviation: 0.012806255810121007\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define the image directory and HOG parameters\n",
    "img_dir = \"C:/WPy64-31110/notebooks/docs/Shoe vs Sandal vs Boot Image Dataset/\"\n",
    "classes = ['boot', 'sandal', 'shoe']\n",
    "winSize = (64,64)\n",
    "blockSize = (16,16)\n",
    "blockStride = (8,8)\n",
    "cellSize = (8,8)\n",
    "nbins = 9\n",
    "\n",
    "# Create the HOG descriptor object\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each image in the directory and extract features\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\")):\n",
    "            # Load the image and resize to a fixed size\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, winSize)\n",
    "\n",
    "                # Compute the HOG descriptor for the image and store in the feature list\n",
    "                features = hog.compute(img)\n",
    "                X.append(features.ravel())\n",
    "\n",
    "                # Extract the label from the filename and store in the label list\n",
    "                label = class_name\n",
    "                y.append(label)\n",
    "\n",
    "# Convert the feature and label lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Create a dictionary that maps class names to integer values\n",
    "class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "\n",
    "# Convert the labels to integer values\n",
    "y = np.array([class_to_idx[label] for label in y])\n",
    "\n",
    "print(\"Feature array shape:\", X.shape)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train / 255.0\n",
    "    X_val = X_val / 255.0\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(X.shape[1],)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "\n",
    "   #Evaluate the model on the validation set\n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Print the accuracy for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "\n",
    "# Print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation:\", np.std(accuracies))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a227ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[57  0  1]\n",
      " [ 0 39  2]\n",
      " [ 0  1 50]]\n",
      "Fold 2 accuracy: 1.000000\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 0 45  0]\n",
      " [ 0  0 59]]\n",
      "Fold 3 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[44  0  0]\n",
      " [ 0 53  2]\n",
      " [ 1  0 50]]\n",
      "Fold 4 accuracy: 0.946667\n",
      "Confusion matrix:\n",
      " [[50  0  0]\n",
      " [ 0 52  1]\n",
      " [ 0  7 40]]\n",
      "Fold 5 accuracy: 0.960000\n",
      "Confusion matrix:\n",
      " [[51  1  1]\n",
      " [ 1 54  2]\n",
      " [ 0  1 39]]\n",
      "Fold 6 accuracy: 0.986667\n",
      "Confusion matrix:\n",
      " [[51  0  0]\n",
      " [ 0 50  0]\n",
      " [ 1  1 47]]\n",
      "Fold 7 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[62  0  0]\n",
      " [ 0 46  2]\n",
      " [ 1  1 38]]\n",
      "Fold 8 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 0 55  1]\n",
      " [ 1  1 46]]\n",
      "Fold 9 accuracy: 0.993333\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 1 44  0]\n",
      " [ 0  0 65]]\n",
      "Fold 10 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[49  0  1]\n",
      " [ 0 49  1]\n",
      " [ 0  2 48]]\n",
      "Overall performance metrics:\n",
      "Accuracy: 0.9766666666666667\n",
      "Confusion matrix:\n",
      " [[496   1   3]\n",
      " [  2 487  11]\n",
      " [  4  14 482]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       500\n",
      "           1       0.97      0.97      0.97       500\n",
      "           2       0.97      0.96      0.97       500\n",
      "\n",
      "    accuracy                           0.98      1500\n",
      "   macro avg       0.98      0.98      0.98      1500\n",
      "weighted avg       0.98      0.98      0.98      1500\n",
      "\n",
      "Cohen's Kappa: 0.965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score\n",
    "\n",
    "# Create empty lists to store the true labels and predicted labels for all folds\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Define the hyperparameters of the MLPClassifier\n",
    "hidden_layer_sizes = (100, 50)\n",
    "activation = 'relu'\n",
    "solver = 'adam'\n",
    "alpha = 0.0001\n",
    "max_iter = 1000\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train an FNN classifier on the training set\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation,\n",
    "                        solver=solver, alpha=alpha, max_iter=max_iter)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Add the true and predicted labels to the lists for all folds\n",
    "    y_true_all.extend(y_val)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "    # Print the accuracy and confusion matrix for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Calculate and print the overall performance metrics\n",
    "print(\"Overall performance metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_true_all, y_pred_all))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true_all, y_pred_all))\n",
    "print(classification_report(y_true_all, y_pred_all))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_true_all, y_pred_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e49e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: [0.994 0.985 0.986]\n",
      "Sensitivity: [0.994 0.972 0.964]\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store the true labels and predicted labels for all folds\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# Define the hyperparameters of the MLPClassifier\n",
    "hidden_layer_sizes = (100, 50)\n",
    "activation = 'relu'\n",
    "solver = 'adam'\n",
    "alpha = 0.0001\n",
    "max_iter = 1000\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train an FNN classifier on the training set\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation,\n",
    "                        solver=solver, alpha=alpha, max_iter=max_iter)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Get the predictions for the validation data\n",
    "    y_pred_fold = clf.predict(X_val)\n",
    "    y_pred_all = np.concatenate([y_pred_all, y_pred_fold])\n",
    "    \n",
    "    # Get the true labels for the validation data\n",
    "    y_true_all = np.concatenate([y_true_all, y_val])\n",
    "\n",
    "# Get the unique labels in y_true_all\n",
    "unique_labels = np.unique(y_true_all)\n",
    "\n",
    "# Check if unique_labels is empty\n",
    "if len(unique_labels) == 0:\n",
    "    print(\"Error: no unique labels in y_true_all\")\n",
    "else:\n",
    "    # Get the confusion matrix\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all, labels=unique_labels)\n",
    "\n",
    "    # Calculate the true positives, false positives, true negatives, and false negatives\n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "    # Calculate the specificity and sensitivity\n",
    "    specificity = TN / (TN + FP)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"Sensitivity:\", sensitivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc250ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'max_iter': 1000, 'solver': 'adam'}\n",
      "Best score: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "# Create an MLPClassifier object\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# Perform grid search with 10-fold cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean test score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11cb36",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae6c1cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (1500, 1764)\n",
      "Label array shape: (1500,)\n",
      "Fold 1 accuracy: 0.960000\n",
      "Confusion matrix:\n",
      " [[57  0  1]\n",
      " [ 0 40  1]\n",
      " [ 0  4 47]]\n",
      "Fold 2 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 1 43  1]\n",
      " [ 1  0 58]]\n",
      "Fold 3 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[44  0  0]\n",
      " [ 1 53  1]\n",
      " [ 1  1 49]]\n",
      "Fold 4 accuracy: 0.940000\n",
      "Confusion matrix:\n",
      " [[50  0  0]\n",
      " [ 0 52  1]\n",
      " [ 1  7 39]]\n",
      "Fold 5 accuracy: 0.960000\n",
      "Confusion matrix:\n",
      " [[51  2  0]\n",
      " [ 0 55  2]\n",
      " [ 2  0 38]]\n",
      "Fold 6 accuracy: 0.980000\n",
      "Confusion matrix:\n",
      " [[51  0  0]\n",
      " [ 0 50  0]\n",
      " [ 1  2 46]]\n",
      "Fold 7 accuracy: 0.953333\n",
      "Confusion matrix:\n",
      " [[62  0  0]\n",
      " [ 1 46  1]\n",
      " [ 4  1 35]]\n",
      "Fold 8 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[46  0  0]\n",
      " [ 0 54  2]\n",
      " [ 1  1 46]]\n",
      "Fold 9 accuracy: 0.960000\n",
      "Confusion matrix:\n",
      " [[40  0  0]\n",
      " [ 0 44  1]\n",
      " [ 2  3 60]]\n",
      "Fold 10 accuracy: 0.973333\n",
      "Confusion matrix:\n",
      " [[49  0  1]\n",
      " [ 0 49  1]\n",
      " [ 0  2 48]]\n",
      "Average accuracy: 0.9653333333333333\n",
      "Standard deviation: 0.012220201853215592\n",
      "Overall performance metrics:\n",
      "Accuracy: 0.9733333333333334\n",
      "Confusion matrix:\n",
      " [[49  0  1]\n",
      " [ 0 49  1]\n",
      " [ 0  2 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        50\n",
      "           1       0.96      0.98      0.97        50\n",
      "           2       0.96      0.96      0.96        50\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n",
      "Cohen's Kappa: 0.96\n",
      "Confusion matrix:\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score\n",
    "# Define the image directory and HOG parameters\n",
    "img_dir = \"C:/WPy64-31110/notebooks/docs/Shoe vs Sandal vs Boot Image Dataset/\"\n",
    "classes = ['boot', 'sandal', 'shoe']\n",
    "winSize = (64,64)\n",
    "blockSize = (16,16)\n",
    "blockStride = (8,8)\n",
    "cellSize = (8,8)\n",
    "nbins = 9\n",
    "\n",
    "# Create the HOG descriptor object\n",
    "hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each image in the directory and extract features\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\")):\n",
    "            # Load the image and resize to a fixed size\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, winSize)\n",
    "\n",
    "                # Compute the HOG descriptor for the image and store in the feature list\n",
    "                features = hog.compute(img)\n",
    "                X.append(features.ravel())\n",
    "\n",
    "                # Extract the label from the filename and store in the label list\n",
    "                label = class_name\n",
    "                y.append(label)\n",
    "\n",
    "# Convert the feature and label lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Create a dictionary that maps class names to integer values\n",
    "class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "\n",
    "# Convert the labels to integer values\n",
    "y = np.array([class_to_idx[label] for label in y])\n",
    "\n",
    "print(\"Feature array shape:\", X.shape)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Create a random forest classifier and train it on the training set\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Print the accuracy and confusion matrix for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation:\", np.std(accuracies))\n",
    "# Calculate and print the overall performance metrics\n",
    "print(\"Overall performance metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1367e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (1500, 1764)\n",
      "Label array shape: (1500,)\n",
      "Fold 1 accuracy: 0.973333\n",
      "Fold 2 accuracy: 0.986667\n",
      "Fold 3 accuracy: 0.973333\n",
      "Fold 4 accuracy: 0.933333\n",
      "Fold 5 accuracy: 0.953333\n",
      "Fold 6 accuracy: 0.980000\n",
      "Fold 7 accuracy: 0.940000\n",
      "Fold 8 accuracy: 0.966667\n",
      "Fold 9 accuracy: 0.960000\n",
      "Fold 10 accuracy: 0.966667\n",
      "Average accuracy: 0.9633333333333335\n",
      "Standard deviation: 0.01612451549659711\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the image directory and HOG parameters\n",
    "img_dir = \"C:/WPy64-31110/notebooks/docs/Shoe vs Sandal vs Boot Image Dataset/\"\n",
    "classes = ['boot', 'sandal', 'shoe']\n",
    "winSize = (64,64)\n",
    "blockSize = (16,16)\n",
    "blockStride = (8,8)\n",
    "cellSize = (8,8)\n",
    "nbins = 9\n",
    "\n",
    "# Create the HOG descriptor object\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each image in the directory and extract features\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\")):\n",
    "            # Load the image and resize to a fixed size\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, winSize)\n",
    "\n",
    "                # Compute the HOG descriptor for the image and store in the feature list\n",
    "                features = hog.compute(img)\n",
    "                X.append(features.ravel())\n",
    "\n",
    "                # Extract the label from the filename and store in the label list\n",
    "                label = class_name\n",
    "                y.append(label)\n",
    "\n",
    "# Convert the feature and label lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Create a dictionary that maps class names to integer values\n",
    "class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "\n",
    "# Convert the labels to integer values\n",
    "y = np.array([class_to_idx[label] for label in y])\n",
    "\n",
    "print(\"Feature array shape:\", X.shape)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "   #Evaluate the model on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Print the accuracy for this fold\n",
    "    print(\"Fold %d accuracy: %f\" % (fold+1, accuracy))\n",
    "\n",
    "# Print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation:\", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e53239dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: boot\n",
      "Sensitivity: 0.992\n",
      "Specificity: 0.982\n",
      "Class: sandal\n",
      "Sensitivity: 0.96\n",
      "Specificity: 0.981\n",
      "Class: shoe\n",
      "Sensitivity: 0.938\n",
      "Specificity: 0.982\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty lists to store true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Loop through each fold in the cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Append the true and predicted labels to the corresponding lists\n",
    "    true_labels.extend(y_val)\n",
    "    predicted_labels.extend(y_pred)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Compute the sensitivity and specificity for each class\n",
    "for i, class_name in enumerate(classes):\n",
    "    tp = cm[i, i]\n",
    "    fn = np.sum(cm[i, :]) - tp\n",
    "    fp = np.sum(cm[:, i]) - tp\n",
    "    tn = np.sum(cm) - tp - fn - fp\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(\"Class:\", class_name)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "    print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ceaaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 200}\n",
      "Best score: 0.9560000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 250]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier object\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=10,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ff788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
